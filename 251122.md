# 251122

## 9:18

tensor autograd nn.Module optim

pytorch

torchvision torchtext torchaudio

torch core

torch.nn torch.optim torch.utils

torch core autograd torch.utils.data

```python
import torch
a = torch.zeros(2, 3)
b = torch.ones(2, 3)
c = torch.randn(2, 3) # 正态分布0，1
d = torch.rand(2, 3) # 均匀分布0，1

import numpy as np
numpy_array = np.array([[1, 2], [3, 4]])
tensor_from_numpy = torch.from_numpy(numpy_array)

device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')
```

```py
e = torch.randn(2, 3)
f = torch.randn(2, 3)
print(e + f)
print(e * f)
g = torch.randn(3, 2)
print(g.t())
print(g.shape)
```

```python
tensor_requires_grad = torch.tensor([1,0], requires_grad=True)
tensor_result = tensor_requires_grad * 2
tensor_result.backward()
print(tensor_requires_grad.grad)
```

```python
x = torch.randn(2, 2, requires_grad=True)
y = x + 2
z = y * y * 3
out = z.mean()

out.backward()
print(x.grad)

with torch.no_grad():
    y = x * 2
```

```python
import torch.nn as nn
import torch.optim as optim

class SimpleNN(nn.Module):
    def __init__(self):
        super(SimpleNN, self).__init__()
        self.fc1 = nn.Linear(2, 2)
        self.fc2 = nn.Linear(2, 1)
    def forward(self, x):
        x = torch.relu(self.fc1(x))
        x = self.fc2(x)
        return x
model = SimpleNN()
print(model)
```

```python
x = torch.randn(1, 2)
output = model(2)
print(output)

criterion = nn.MSELoss()
target = torch.randn(1, 1)
loss = criterion(output, target)
print(loss)
```

```python
optimizer = optim.Adam(model.parameters(), lr=0.001)
optimizer.zero_grad()
loss.backward()
optimizer.step()
```

```python
import torch
import torch.nn as nn
import torch.optim as optim
class SimpleNN(nn.Module):
    def __init__(self):
        super(SimpleNN, self).__init__()
        self.fc1 = nn.Linear(2, 2)
        self.fc2 = nn.Linear(2, 1)
    def forward(self, x):
        x = torch.relu(self.fc1(x))
        x = self.fc2(x)
        return x
model = SimpleNN()
criterion = nn.MSELoss()
optimizer = optim.Adam(model.parameters(), lr=0.001)
X = torch.randn(10, 2)
Y = torch.randn(10, 1)
for epoch in range(100):
    optimizer.zero_grad()
    output = model(X)
    loss = criterion(output, Y)
    loss.backward()
    optimizer.step()
    if (epoch+1) % 10 == 0:
        print(f'Epoch: [{epoch+1}/100], Loss: {loss.item():.4f}')
```

```python
device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')
model.to(device)
X = X.to(device)
Y = Y.to(device)
```